{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95181e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats on Ca2+ imaging with miniscope and Vigilance States\n",
    "\n",
    "#######################################################################################\n",
    "                            # Define Experiment type #\n",
    "#######################################################################################\n",
    "\n",
    "AnalysisID='_baseline' #to identify this analysis from another\n",
    "DrugExperiment=0 # 0 if Baseline, 1 if CGP, 2 if Baseline & CGP\n",
    "\n",
    "saveexcel=0\n",
    "Local=1\n",
    "\n",
    "choosed_folder1='VigSt_2025-05-03_10_01_32' # for Baseline Expe\n",
    "choosed_folder2='VigSt_2025-05-21_15_47_42_CGP' # for CGP Expe\n",
    "\n",
    "desired_order = ['AW','QW', 'NREM', 'IS', 'REM', 'undefined']   \n",
    "\n",
    "#######################################################################################\n",
    "                                # Load packages #\n",
    "#######################################################################################\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import quantities as pq\n",
    "import numpy as np\n",
    "import math \n",
    "import neo\n",
    "import json\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider, Button, Cursor\n",
    "import pickle\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from scipy.stats import ttest_ind\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "class Tee:\n",
    "    def __init__(self, *files):\n",
    "        self.files = files\n",
    "    def write(self, obj):\n",
    "        for f in self.files:\n",
    "            f.write(obj)\n",
    "            f.flush()\n",
    "    def flush(self):\n",
    "        for f in self.files:\n",
    "            f.flush()\n",
    "\n",
    "def divide_keys(data, startkey, everykey):\n",
    "    for it in range(startkey, len(data), everykey):        \n",
    "        key2 = list(data.keys())[it-1]\n",
    "        key3 = list(data.keys())[it]\n",
    "        d=data[key3]\n",
    "        data[key3]=d.replace(0, np.nan)\n",
    "        if startkey>1:\n",
    "            key1 = list(data.keys())[it-2]\n",
    "            data[key1] = data[key1] / data[key3]\n",
    "        data[key2] = data[key2] / data[key3]\n",
    "    keys_to_delete = list(data.keys())[startkey::everykey]\n",
    "    for key in keys_to_delete:\n",
    "        del data[key]\n",
    "    return data   \n",
    "\n",
    "########################################################################\n",
    "        # SCRIPT 23AB_GrandAverages&Stats_for_VigilanceStates\n",
    "########################################################################\n",
    "\n",
    "# Specify the directory containing the Excel files\n",
    "InitialDirectory1 = \"//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/Analysed2025_AB/_baseline_analysis\" if Local else \"/crnldata/waking/audrey_hay/L1imaging/Analysed2025_AB/_baseline_analysis\" \n",
    "directory1= f'{InitialDirectory1}/{choosed_folder1}'\n",
    "InitialDirectory2 =\"//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/Analysed2025_AB/_CGP_analysis\" if Local else \"/crnldata/waking/audrey_hay/L1imaging/Analysed2025_AB/_CGP_analysis\"\n",
    "directory2= f'{InitialDirectory2}/{choosed_folder2}'\n",
    "\n",
    "# Get the current date and time\n",
    "FolderNameSave=str(datetime.now())[:19]\n",
    "FolderNameSave = FolderNameSave.replace(\" \", \"_\").replace(\".\", \"_\").replace(\":\", \"_\")\n",
    "destination_folder= f\"//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/Analysed2025_AB/_global_analysis/Corr_VigSt_{FolderNameSave}{AnalysisID}\" if Local else f\"/crnldata/waking/audrey_hay/L1imaging/Analysed2025_AB/_global_analysis/Corr_VigSt_{FolderNameSave}{AnalysisID}\"\n",
    "os.makedirs(destination_folder)\n",
    "folder_to_save=Path(destination_folder)\n",
    "\n",
    "# Copy the script file to the destination folder\n",
    "source_script = \"C:/Users/Manip2/SCRIPTS/CodePythonAudrey/CodePythonAurelie/HayLabAnalysis/python/_MINI&OE_3_average_correlations.py\" if Local else \"/python/_MINI&OE_3_average_correlations.py\" \n",
    "destination_file_path = f\"{destination_folder}/_MINI&OE_3_average_correlations.txt\"\n",
    "shutil.copy(source_script, destination_file_path)\n",
    "\n",
    "logfile = open(f\"{destination_folder}/output_log.txt\", 'w')\n",
    "sys.stdout = Tee(sys.stdout, logfile)  # print goes to both\n",
    "\n",
    "\n",
    "directories= [directory1, directory2] if DrugExperiment else [directory1]\n",
    "#directories= [directory2] \n",
    "\n",
    "NrSubtypeList=['L1NDNF_mice','L2_3_mice']\n",
    "\n",
    "for NrSubtype in NrSubtypeList:  \n",
    "    dfs2_per_sheet = {}\n",
    "    dfs3_per_sheet = {}\n",
    "    dfs4_per_sheet = {}\n",
    "    dfs5_per_sheet = {}\n",
    "    dfs6_per_sheet = {}\n",
    "\n",
    "    if NrSubtype=='L1NDNF_mice':\n",
    "        MiceList=['BlackLines', 'BlueLines', 'GreenDots', 'GreenLines', 'RedLines']\n",
    "    else:\n",
    "        MiceList=['PurpleSquare', 'ThreeColDots', 'ThreeBlueCrosses']\n",
    "    \n",
    "    nametofind2='VigSt_CaCorr'\n",
    "    nametofind3='VigSt_SpCorr'      \n",
    "    nametofind4='TotCaCorr'\n",
    "    nametofind5='TotSpCorr'\n",
    "    nametofind6='StatesCaCorr'\n",
    "\n",
    "    # Recursively traverse the directory structure\n",
    "    for directory in directories:\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for filename in files:\n",
    "                # Check if the file is an pkl file and contains the specified name\n",
    "                if filename.endswith('.pkl') and nametofind2 in filename: \n",
    "                    if any(name in filename for name in MiceList): \n",
    "                        # Construct the full path to the file\n",
    "                        filepath = os.path.join(root, filename)\n",
    "                        with open(filepath, 'rb') as pickle_file:\n",
    "                            df = pickle.load(pickle_file)\n",
    "                        for key, value in df.items():\n",
    "                            if key in dfs2_per_sheet:\n",
    "                                dfs2_per_sheet[key]=pd.concat([dfs2_per_sheet[key],value],axis=0)\n",
    "                            else:\n",
    "                                dfs2_per_sheet[key]=value\n",
    "                        print(filename)\n",
    "                if filename.endswith('.pkl') and nametofind3 in filename: \n",
    "                    if any(name in filename for name in MiceList): \n",
    "                        # Construct the full path to the file\n",
    "                        filepath = os.path.join(root, filename)\n",
    "                        with open(filepath, 'rb') as pickle_file:\n",
    "                            df = pickle.load(pickle_file)\n",
    "                        for key, value in df.items():\n",
    "                            if key in dfs3_per_sheet:\n",
    "                                dfs3_per_sheet[key]=pd.concat([dfs3_per_sheet[key],value],axis=0)\n",
    "                            else:\n",
    "                                dfs3_per_sheet[key]=value\n",
    "                        print(filename)\n",
    "                if filename.endswith('.pkl') and nametofind4 in filename: \n",
    "                    if any(name in filename for name in MiceList): \n",
    "                        # Construct the full path to the file\n",
    "                        filepath = os.path.join(root, filename)\n",
    "                        with open(filepath, 'rb') as pickle_file:\n",
    "                            df = pickle.load(pickle_file)\n",
    "                        for key, value in df.items():\n",
    "                            if key in dfs4_per_sheet:\n",
    "                                dfs4_per_sheet[key]=pd.concat([dfs4_per_sheet[key],value],axis=0)\n",
    "                            else:\n",
    "                                dfs4_per_sheet[key]=value\n",
    "                        print(filename)\n",
    "                if filename.endswith('.pkl') and nametofind5 in filename: \n",
    "                    if any(name in filename for name in MiceList): \n",
    "                        # Construct the full path to the file\n",
    "                        filepath = os.path.join(root, filename)\n",
    "                        with open(filepath, 'rb') as pickle_file:\n",
    "                            df = pickle.load(pickle_file)\n",
    "                        for key, value in df.items():\n",
    "                            if key in dfs5_per_sheet:\n",
    "                                dfs5_per_sheet[key]=pd.concat([dfs5_per_sheet[key],value],axis=0)\n",
    "                            else:\n",
    "                                dfs5_per_sheet[key]=value\n",
    "                        print(filename)\n",
    "                if filename.endswith('.pkl') and nametofind6 in filename: \n",
    "                    if any(name in filename for name in MiceList): \n",
    "                        # Construct the full path to the file\n",
    "                        filepath = os.path.join(root, filename)\n",
    "                        with open(filepath, 'rb') as pickle_file:\n",
    "                            try : df = pickle.load(pickle_file)\n",
    "                            except: pass\n",
    "                        for key, value in df.items():\n",
    "                            if key in dfs6_per_sheet:\n",
    "                                dfs6_per_sheet[key]=pd.concat([dfs6_per_sheet[key],value],axis=0)\n",
    "                            else:\n",
    "                                dfs6_per_sheet[key]=value\n",
    "                        print(filename)\n",
    "\n",
    "    ######### Save the SubStates Ca correlation matrix   ########\n",
    "\n",
    "    dfs6_per_sheet = {sheet_name: df.groupby(df.index).sum() for sheet_name, df in dfs6_per_sheet.items()} #cause was concatenated in the 0 axis\n",
    "    dfs6_per_sheet=divide_keys(dfs6_per_sheet, 1, 2)\n",
    "    for sheet_name, df in dfs6_per_sheet.items():\n",
    "        df = df.sort_index(axis=1)\n",
    "        df = df.sort_index(axis=0)\n",
    "        dfs6_per_sheet[sheet_name]=df\n",
    "\n",
    "    if saveexcel:\n",
    "        file_path = f'{folder_to_save}/{NrSubtype}_SubSt_CaCorr.xlsx'\n",
    "        with pd.ExcelWriter(file_path) as writer:        \n",
    "            for sheet_name, df in dfs6_per_sheet.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=True, header=True)\n",
    "\n",
    "    filenameOut = f'{folder_to_save}/{NrSubtype}_SubSt_CaCorr.pkl'\n",
    "    with open(filenameOut, 'wb') as pickle_file:\n",
    "        pickle.dump(dfs6_per_sheet, pickle_file)\n",
    "\n",
    "    ######### Save the Ca correlation matrix   ########\n",
    "\n",
    "    dfs2_per_sheet = {sheet_name: df.groupby(df.index).sum() for sheet_name, df in dfs2_per_sheet.items()} #cause was concatenated in the 0 axis\n",
    "    dfs2_per_sheet=divide_keys(dfs2_per_sheet, 2, 3)\n",
    "    for sheet_name, df in dfs2_per_sheet.items():\n",
    "        df = df.sort_index(axis=1)\n",
    "        df = df.sort_index(axis=0)\n",
    "        dfs2_per_sheet[sheet_name]=df\n",
    "\n",
    "    if saveexcel:\n",
    "        file_path = f'{folder_to_save}/{NrSubtype}_VigSt_CaCorr.xlsx'\n",
    "        with pd.ExcelWriter(file_path) as writer:        \n",
    "            for sheet_name, df in dfs2_per_sheet.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=True, header=True)\n",
    "\n",
    "    filenameOut = f'{folder_to_save}/{NrSubtype}_VigSt_CaCorr.pkl'\n",
    "    with open(filenameOut, 'wb') as pickle_file:\n",
    "        pickle.dump(dfs2_per_sheet, pickle_file)\n",
    "\n",
    "    ######### Save the Sp correlation matrix  ########\n",
    "\n",
    "    dfs3_per_sheet = {sheet_name: df.groupby(df.index).sum() for sheet_name, df in dfs3_per_sheet.items()}\n",
    "    dfs3_per_sheet=divide_keys(dfs3_per_sheet, 2, 3)\n",
    "    for sheet_name, df in dfs3_per_sheet.items():\n",
    "        df = df.sort_index(axis=1)\n",
    "        df = df.sort_index(axis=0)\n",
    "        dfs3_per_sheet[sheet_name]=df\n",
    "\n",
    "    if saveexcel:    \n",
    "        file_path = f'{folder_to_save}/{NrSubtype}_VigSt_SpCorr.xlsx'\n",
    "        with pd.ExcelWriter(file_path) as writer:        \n",
    "            for sheet_name, df in dfs3_per_sheet.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=True, header=True)\n",
    "\n",
    "    filenameOut = f'{folder_to_save}/{NrSubtype}_VigSt_SpCorr.pkl'\n",
    "    with open(filenameOut, 'wb') as pickle_file:\n",
    "        pickle.dump(dfs3_per_sheet, pickle_file)\n",
    "\n",
    "    ####### Save the TOT Ca correlation matrix  ########\n",
    "\n",
    "    dfs4_per_sheet = {sheet_name: df.groupby(df.index).sum() for sheet_name, df in dfs4_per_sheet.items()} #cause was concatenated in the 0 axis\n",
    "    dfs4_per_sheet=divide_keys(dfs4_per_sheet, 2, 3)\n",
    "    for sheet_name, df in dfs4_per_sheet.items():\n",
    "        df = df.sort_index(axis=1)\n",
    "        df = df.sort_index(axis=0)\n",
    "        dfs4_per_sheet[sheet_name]=df\n",
    "\n",
    "    if saveexcel:\n",
    "        file_path = f'{folder_to_save}/{NrSubtype}_Tot_CaCorr.xlsx'      \n",
    "        with pd.ExcelWriter(file_path) as writer:  \n",
    "            for sheet_name, df in dfs4_per_sheet.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=True, header=True)\n",
    "\n",
    "    filenameOut = f'{folder_to_save}/{NrSubtype}_Tot_CaCorr.pkl'\n",
    "    with open(filenameOut, 'wb') as pickle_file:\n",
    "        pickle.dump(dfs4_per_sheet, pickle_file)\n",
    "\n",
    "    ######### Save the TOT Sp correlation matrix   ########\n",
    "\n",
    "    dfs5_per_sheet = {sheet_name: df.groupby(df.index).sum() for sheet_name, df in dfs5_per_sheet.items()}\n",
    "    dfs5_per_sheet=divide_keys(dfs5_per_sheet, 2, 3)\n",
    "    for sheet_name, df in dfs5_per_sheet.items():\n",
    "        df = df.sort_index(axis=1)\n",
    "        df = df.sort_index(axis=0)\n",
    "        dfs5_per_sheet[sheet_name]=df\n",
    "\n",
    "    if saveexcel:   \n",
    "        file_path = f'{folder_to_save}/{NrSubtype}_Tot_SpCorr.xlsx' \n",
    "        with pd.ExcelWriter(file_path) as writer:        \n",
    "            for sheet_name, df in dfs5_per_sheet.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=True, header=True)\n",
    "\n",
    "    filenameOut = f'{folder_to_save}/{NrSubtype}_Tot_SpCorr.pkl'\n",
    "    with open(filenameOut, 'wb') as pickle_file:\n",
    "        pickle.dump(dfs5_per_sheet, pickle_file)\n",
    "\n",
    "sys.stdout = sys.__stdout__\n",
    "logfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
